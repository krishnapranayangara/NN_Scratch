# -*- coding: utf-8 -*-
"""krishnapranay_angara_a1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oiGbpac7_MoSyYq5Gc_yYZ25ajuxKCpM

# Libraries
"""

from google.colab import drive
drive.mount('/content/MyDrive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

"""# Data Pre-processing"""

data = pd.read_pickle("/content/MyDrive/MyDrive/assignment-1/cifar_2class_py2.p")

train_x = data['train_data']
train_y = data['train_labels']
test_x = data['test_data']
test_y = data['test_labels']

train_x = train_x / 255.0
test_x = test_x / 255.0

train_y = train_y.reshape(-1, 1)
test_y = test_y.reshape(-1, 1)

"""# Linear transform"""

class LinearTransform(object):
    def __init__(self, W, b):
        self.W = W
        self.b = b
        self.velW = np.zeros_like(W)
        self.velb = np.zeros_like(b)

    def forward(self, x):
        self.x = x
        return np.dot(x, self.W) + self.b

    def backward(self, grad_output, learning_rate=0.0, momentum=0.0, l2_penalty=0.0):
        #Legularization --> l2_penalty*self.w add to grad weight
        dW = np.dot(self.x.T, grad_output) / self.x.shape[0] + l2_penalty * self.W
        db = np.sum(grad_output, axis=0, keepdims=True) / self.x.shape[0] +l2_penalty * self.b
        dx = np.dot(grad_output, self.W.T)

        # print("dx shape ",dx.shape,"db shape: ",db.shape)
        #vw = momentum*vw + (1-momentum)*dw
        self.velW = momentum * self.velW + (1 - momentum) * dW
        #vb = momentum*vb + (1-momentum)*db
        self.velb = momentum * self.velb + (1 - momentum) * db

        #W = alpha * vw
        self.W -= learning_rate * self.velW
        #b = alpha * vb
        self.b -= learning_rate * self.velb

        return dx

"""# Loss"""

class ReLU(object):
    def forward(self, x):
        self.x = x
        return np.maximum(0, x)

    def backward(self, grad_output, learning_rate=0.0, momentum=0.0, l2_penalty=0.0):
        return grad_output * (self.x > 0)

class Sigmoid(object):
    def forward(self, x):
        self.x = x
        return 1 / (1 + np.exp(-x))

    def backward(self, y, learning_rate=0.0, momentum=0.0, l2_penalty=0.0):
        y_hat = self.forward(self.x)
        return y_hat - y

"""# Model"""

class Net(object):
    def __init__(self, input_dims, hidden_units, learning_rate=0.01, l2_penalty=0.01, momentum=0.9):
        #load parameters
        self.learning_rate = learning_rate
        self.l2_penalty = l2_penalty
        self.momentum = momentum
        #(3073,hidden_neurons)
        self.linear1 = LinearTransform(np.random.randn(input_dims, hidden_units) * 0.01, np.zeros((1, hidden_units)))
        #final output is only 1
        self.linear2 = LinearTransform(np.random.randn(hidden_units, 1) * 0.01, np.zeros((1, 1)))
        self.relu = ReLU()
        self.sigmoid = Sigmoid()

    def train(self, x_batch, y_batch):
      #---------------forward pass----------------
        #z1 = w1x+b
        z1 = self.linear1.forward(x_batch)
        #a1 = relu(z1)
        a1 = self.relu.forward(z1)
        #z2 = w2(a1)+b2
        z2 = self.linear2.forward(a1)
        #a2 = sigmoid(z2)
        a2 = self.sigmoid.forward(z2)

        #--------------backward pass---------------
        #der_J/der_a2  --> (a2-y_batch) --> loss.backward
        #der_a2/der_z2 --> a2*(1-a2)    --> sigmoid.backward
        #der_z2/der_a1 --> w2           --> linear2.backward
        #der_a1/ser_z1 --> (x>0)        --> relu.backward
        #der_z1/der_w1 --> x            --> linear1.backward

        loss_grad = self.sigmoid.backward(y_batch)
        grad_z2 = self.linear2.backward(loss_grad, self.learning_rate, self.momentum, self.l2_penalty)
        grad_a1 = self.relu.backward(grad_z2)
        self.linear1.backward(grad_a1, self.learning_rate, self.momentum, self.l2_penalty)

    def evaluate(self, x, y):
        z1 = self.linear1.forward(x)
        a1 = self.relu.forward(z1)
        z2 = self.linear2.forward(a1)
        a2 = self.sigmoid.forward(z2)
        predictions = a2 > 0.5
        accuracy = np.mean(predictions == y)
        return accuracy

    def cross_entropy(self, a2, y):
        loss = -np.mean(y * np.log(a2) + (1 - y) * np.log(1 - a2))
        return loss

"""# Q4. Training and evaluation"""

num_epochs = 10
input_dims = train_x.shape[1]

#hyperparameters
hidden_units = 15
learning_rate=0.01
num_batches = 200
momentum=0.8
l2_penalty=0.0001

#model
nnet = Net(input_dims, hidden_units, learning_rate=0.01, l2_penalty=0.0001, momentum=0.8)

#mini-batch
batch_size = train_x.shape[0] // num_batches


cross_entropy_losses = []
Total_losses         = []
Train_errors         = []
Test_errors          = []

for epoch in range(num_epochs):
    total_loss = 0.0
    for b in range(num_batches):
        batch_start = b * batch_size
        batch_end = (b + 1) * batch_size

        x_batch = train_x[batch_start:batch_end, :]
        y_batch = train_y[batch_start:batch_end, :]

        #Training the model
        nnet.train(x_batch, y_batch)
        #Find the predictions(output of a2)
        predictions = nnet.sigmoid.forward(nnet.linear2.forward(nnet.relu.forward(nnet.linear1.forward(x_batch))))
        #loss function
        batch_loss = nnet.cross_entropy(predictions, y_batch)
        total_loss += batch_loss

        print('\r[Epoch {}, mb {}]    Avg.Loss = {:.3f}'.format(epoch + 1,b + 1, total_loss / (b + 1),),end='')

    avg_loss = total_loss / num_batches
    Total_losses.append(avg_loss)
    #error values
    train_error = 1 - nnet.evaluate(train_x, train_y)
    test_error = 1 - nnet.evaluate(test_x, test_y)
    Train_errors.append(train_error)
    Test_errors.append(test_error)
    #loss values
    train_loss = nnet.cross_entropy(nnet.sigmoid.forward(nnet.linear2.forward(nnet.relu.forward(nnet.linear1.forward(train_x)))), train_y)
    test_loss = nnet.cross_entropy(nnet.sigmoid.forward(nnet.linear2.forward(nnet.relu.forward(nnet.linear1.forward(test_x)))), test_y)
    cross_entropy_losses.append(train_loss)
    print()
    print(' Train Loss: {:.3f}       Train Error: {:.2f}%'.format(train_loss, 100. * train_error))
    print(' Test Loss:  {:.3f}       Test Error:  {:.2f}%'.format(test_loss, 100. * test_error))
    print(f'-------------------> Train Accuracy: {(1-train_error) * 100:.2f}%, Test Accuracy: {(1-test_error) * 100:.2f}%')

"""##Plots"""

plt.plot(cross_entropy_losses,range(0,len(cross_entropy_losses)))
plt.xlabel("Cross Entropy Losses")
plt.ylabel("Epochs")
plt.title("Cross entropy Losses of training data Plot")
plt.grid()

plt.plot(Train_errors,range(0,len(Train_errors)))
plt.xlabel("Train Errors")
plt.ylabel("Epochs")
plt.title("Train Error Plot")
plt.grid()

plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Test Errors")
plt.ylabel("Epochs")
plt.title("Test Error Plot")
plt.grid()

plt.plot(Total_losses,range(0,len(Total_losses)))
plt.xlabel("Total Losses")
plt.ylabel("Epochs")
plt.title("Total loss Plot")
plt.grid()

"""# Q5. plots"""

def main(hidden_units,learning_rate,num_batches,momentum,l2_penalty):
  num_epochs = 50
  input_dims = train_x.shape[1]
  #model
  nnet = Net(input_dims, hidden_units, learning_rate, l2_penalty, momentum)
  #mini-batch
  batch_size = train_x.shape[0] // num_batches
  Test_error_losses = []

  for epoch in range(num_epochs):
      total_loss = 0.0
      for b in range(num_batches):
          batch_start = b * batch_size
          batch_end = (b + 1) * batch_size

          x_batch = train_x[batch_start:batch_end, :]
          y_batch = train_y[batch_start:batch_end, :]

          #Training the model
          nnet.train(x_batch, y_batch)
          #Find the predictions(output of a2)
          predictions = nnet.sigmoid.forward(nnet.linear2.forward(nnet.relu.forward(nnet.linear1.forward(x_batch))))
          #loss function
          batch_loss = nnet.cross_entropy(predictions, y_batch)

      test_error = 1 - nnet.evaluate(test_x, test_y)
      Test_error_losses.append(test_error)

  print("Accuracy: ",(1-test_error)*100,"%")
  return Test_error_losses

"""# Hidden units Plot (5,10,15,20,25,30)"""

Test_errors=main(hidden_units=5,learning_rate=0.01,num_batches=200,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("Hidden Units - 5")
plt.grid()

Test_errors=main(hidden_units=10,learning_rate=0.01,num_batches=200,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("Hidden Units - 10")
plt.grid()

Test_errors=main(hidden_units=15,learning_rate=0.01,num_batches=200,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("Hidden Units - 15")
plt.grid()

Test_errors=main(hidden_units=20,learning_rate=0.01,num_batches=200,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("Hidden Units - 20")
plt.grid()

Test_errors=main(hidden_units=25,learning_rate=0.01,num_batches=200,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("Hidden Units - 25")
plt.grid()

Test_errors=main(hidden_units=30,learning_rate=0.01,num_batches=200,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("Hidden Units - 30")
plt.grid()

"""# Learning rate Plots (0.001,0.01, 0.03,0.05,0.1)"""

Test_errors=main(hidden_units=15,learning_rate=0.001,num_batches=200,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("lr - 0.001")
plt.grid()

Test_errors=main(hidden_units=15,learning_rate=0.01,num_batches=200,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("lr - 0.01")
plt.grid()

Test_errors=main(hidden_units=15,learning_rate=0.03,num_batches=200,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("lr - 0.03")
plt.grid()

Test_errors=main(hidden_units=15,learning_rate=0.05,num_batches=200,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("lr - 0.05")
plt.grid()

Test_errors=main(hidden_units=15,learning_rate=0.1,num_batches=200,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("lr - 0.1")
plt.grid()

"""# Mini-batch size Plots (10,50,100,200,500)"""

Test_errors=main(hidden_units=15,learning_rate=0.01,num_batches=10,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("Mini batch - 10")
plt.grid()

Test_errors=main(hidden_units=15,learning_rate=0.01,num_batches=50,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("Mini batch - 50")
plt.grid()

Test_errors=main(hidden_units=15,learning_rate=0.01,num_batches=100,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("Mini batch - 100")
plt.grid()

Test_errors=main(hidden_units=15,learning_rate=0.01,num_batches=200,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("Mini batch - 200")
plt.grid()

Test_errors=main(hidden_units=15,learning_rate=0.01,num_batches=500,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("Mini batch - 500")
plt.grid()

"""# Momentum parameter Plots (0.8,0.85,0.9)"""

Test_errors=main(hidden_units=15,learning_rate=0.01,num_batches=200,momentum=0.8,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("Momentum - 0.8")
plt.grid()

Test_errors=main(hidden_units=15,learning_rate=0.01,num_batches=200,momentum=0.85,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("Momentum - 0.85")
plt.grid()

Test_errors=main(hidden_units=15,learning_rate=0.01,num_batches=200,momentum=0.9,l2_penalty=0.0001)
plt.plot(Test_errors,range(0,len(Test_errors)))
plt.xlabel("Losses")
plt.ylabel("Epochs")
plt.title("Momentum - 0.9")
plt.grid()